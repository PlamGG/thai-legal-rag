import gradio as gr
import pickle
import chromadb
import numpy as np
import torch
import re
import json
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer
from peft import PeftModel
from rank_bm25 import BM25Okapi
import time
import os
import sys

# Legal keywords
LEGAL_KEYWORDS = {
    '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏´‡∏∏‡πâ‡∏ô', '‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£', '‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢', '‡∏°‡∏≤‡∏ï‡∏£‡∏≤', '‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°', '‡∏†‡∏≤‡∏©‡∏µ', '‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï', '‡∏õ.‡∏õ.‡∏ä.', '‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à',
    '‡∏™‡∏±‡∏ç‡∏ç‡∏≤', '‡∏ô‡∏¥‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°', '‡∏°‡∏±‡∏î‡∏à‡∏≥', '‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô', '‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô', '‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏Ñ‡∏ô‡∏ï‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ß', '‡∏≠‡∏ô‡∏∏‡∏£‡∏±‡∏Å‡∏©‡πå',
    '‡∏™‡∏°‡∏≤‡∏Ñ‡∏°', '‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ò‡∏¥', '‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô', '‡∏ó‡∏£‡∏±‡∏™‡∏ï‡πå', '‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô', '‡∏ó‡∏∏‡∏ô', '‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', '‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô', '‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå',
    '‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏´‡∏∏‡πâ‡∏ô', '‡πÄ‡∏á‡∏¥‡∏ô‡∏õ‡∏±‡∏ô‡∏ú‡∏•', '‡πÉ‡∏ö‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï', '‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£', '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á', '‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô', '‡∏õ‡∏£‡∏≤‡∏ö‡∏õ‡∏£‡∏≤‡∏°', '‡πÄ‡∏á‡∏¥‡∏ô‡πÑ‡∏î‡πâ',
    '‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û', '‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì', '‡∏ß‡∏¥‡∏ô‡∏±‡∏¢', '‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô', '‡∏Ñ‡∏•‡∏±‡∏á', '‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î', '‡∏Ç‡πâ‡∏≤‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£', '‡∏û‡∏±‡∏í‡∏ô‡∏≤',
    '‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à', '‡∏´‡∏•‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô', '‡∏´‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤', '‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô', '‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå'
}

# Legal categories
LEGAL_CATEGORIES = {
    "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏û‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå": "‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏™‡∏±‡∏ç‡∏ç‡∏≤ ‡∏°‡∏±‡∏î‡∏à‡∏≥ ‡∏ô‡∏¥‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏° ‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏™‡∏¥‡∏ô",
    "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏≠‡∏≤‡∏ç‡∏≤": "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏≠‡∏≤‡∏ç‡∏≤ ‡πÇ‡∏ó‡∏© ‡∏Å‡∏≤‡∏£‡∏ü‡πâ‡∏≠‡∏á‡∏£‡πâ‡∏≠‡∏á‡∏Ñ‡∏î‡∏µ‡∏≠‡∏≤‡∏ç‡∏≤",
    "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏û‡πà‡∏á": "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏î‡∏µ‡πÅ‡∏û‡πà‡∏á",
    "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏≤‡∏ç‡∏≤": "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Ñ‡∏î‡∏µ‡∏≠‡∏≤‡∏ç‡∏≤",
    "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏£‡∏±‡∏©‡∏é‡∏≤‡∏Å‡∏£": "‡∏†‡∏≤‡∏©‡∏µ‡πÄ‡∏á‡∏¥‡∏ô‡πÑ‡∏î‡πâ ‡∏†‡∏≤‡∏©‡∏µ‡∏°‡∏π‡∏•‡∏Ñ‡πà‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏° ‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡πà‡∏ô‡∏†‡∏≤‡∏©‡∏µ",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏°‡∏´‡∏≤‡∏ä‡∏ô‡∏à‡∏≥‡∏Å‡∏±‡∏î ‡∏û.‡∏®. 2535": "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏°‡∏´‡∏≤‡∏ä‡∏ô",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå ‡∏û.‡∏®. 2535": "‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå ‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏∏‡πâ‡∏ô",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏î‡∏π‡πÅ‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à ‡∏û.‡∏®. 2562": "‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç‡∏ß‡πà‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏≤‡∏ö‡∏õ‡∏£‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï ‡∏û.‡∏®. 2561": "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà ‡∏õ.‡∏õ.‡∏ä. ‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå ‡∏û.‡∏®. 2518": "‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πÅ‡∏•‡∏∞‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏ô‡∏≤‡∏¢‡∏à‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏•‡∏π‡∏Å‡∏à‡πâ‡∏≤‡∏á",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô ‡∏û.‡∏®. 2541": "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏û‡∏ô‡∏±‡∏ô ‡∏û.‡∏®. 2478": "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏ô‡∏±‡∏ô",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏¢‡∏≤‡πÄ‡∏™‡∏û‡∏ï‡∏¥‡∏î‡πÉ‡∏´‡πâ‡πÇ‡∏ó‡∏© ‡∏û.‡∏®. 2522": "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏¢‡∏≤‡πÄ‡∏™‡∏û‡∏ï‡∏¥‡∏î",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏Ñ‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡∏û.‡∏®. 2522": "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ô‡∏ï‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ß ‡∏ß‡∏µ‡∏ã‡πà‡∏≤",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß ‡∏û.‡∏®. 2478": "‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏™‡∏°‡∏£‡∏™ ‡∏´‡∏¢‡πà‡∏≤",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå ‡∏û.‡∏®. 2537": "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á‡∏•‡∏¥‡∏Ç‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤ ‡∏û.‡∏®. 2534": "‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏ö‡∏±‡∏ï‡∏£ ‡∏û.‡∏®. 2522": "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∏‡πâ‡∏°‡∏Ñ‡∏£‡∏≠‡∏á‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏ö‡∏±‡∏ï‡∏£",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ç‡∏ä‡∏µ ‡∏û.‡∏®. 2543": "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏ö‡∏±‡∏ç‡∏ä‡∏µ ‡∏á‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏™‡∏°‡∏≤‡∏Ñ‡∏° ‡∏û.‡∏®. 2499": "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏™‡∏°‡∏≤‡∏Ñ‡∏°",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ò‡∏¥ ‡∏û.‡∏®. 2535": "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ò‡∏¥",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ô‡∏¥‡∏ß‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏±‡∏ô‡∏ï‡∏¥ ‡∏û.‡∏®. 2559": "‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ô‡∏¥‡∏ß‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô ‡∏û.‡∏®. 2550": "‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ú‡∏±‡∏á‡πÄ‡∏°‡∏∑‡∏≠‡∏á ‡∏û.‡∏®. 2562": "‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡∏ú‡∏±‡∏á‡πÄ‡∏°‡∏∑‡∏≠‡∏á",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£ ‡∏û.‡∏®. 2522": "‡∏Å‡∏≤‡∏£‡∏Å‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏≠‡∏≤‡∏Ñ‡∏≤‡∏£",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏£‡∏±‡∏Å‡∏©‡πå‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô ‡∏û.‡∏®. 2535": "‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏£‡∏±‡∏Å‡∏©‡πå‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡πÇ‡∏£‡∏á‡∏á‡∏≤‡∏ô ‡∏û.‡∏®. 2535": "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡πÇ‡∏£‡∏á‡∏á‡∏≤‡∏ô",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÅ‡∏´‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡∏û.‡∏®. 2485": "‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÅ‡∏´‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏±‡∏á‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê ‡∏û.‡∏®. 2561": "‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏£‡∏±‡∏ê",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏ß‡∏¥‡∏ô‡∏±‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏±‡∏á‡∏Ç‡∏≠‡∏á‡∏£‡∏±‡∏ê ‡∏û.‡∏®. 2561": "‡∏ß‡∏¥‡∏ô‡∏±‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏±‡∏ê",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏´‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤ ‡∏û.‡∏®. 2509": "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏´‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏û‡∏±‡∏™‡∏î‡∏∏‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê ‡∏û.‡∏®. 2560": "‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏£‡∏±‡∏ê",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡∏Ñ‡∏° ‡∏û.‡∏®. 2560": "‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•",
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏´‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞ ‡∏û.‡∏®. 2548": "‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏´‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏∞"
}

# Example questions
example_questions = {
    "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏û‡πà‡∏á‡πÅ‡∏•‡∏∞‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå": ["‡∏°‡∏±‡∏î‡∏à‡∏≥‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£", "‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£"],
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏î‡∏π‡πÅ‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à ‡∏û.‡∏®. 2562": ["‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°‡∏Ç‡∏≠‡∏á‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à"],
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç‡∏ß‡πà‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏≤‡∏ö‡∏õ‡∏£‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï ‡∏û.‡∏®. 2561": ["‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á ‡∏õ.‡∏õ.‡∏ä.", "‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï"],
    "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏£‡∏±‡∏©‡∏é‡∏≤‡∏Å‡∏£": ["‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡πà‡∏ô‡∏†‡∏≤‡∏©‡∏µ‡πÄ‡∏á‡∏¥‡∏ô‡πÑ‡∏î‡πâ‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏ò‡∏£‡∏£‡∏°‡∏î‡∏≤"],
    "‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏°‡∏´‡∏≤‡∏ä‡∏ô‡∏à‡∏≥‡∏Å‡∏±‡∏î ‡∏û.‡∏®. 2535": ["‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏°‡∏´‡∏≤‡∏ä‡∏ô‡∏à‡∏≥‡∏Å‡∏±‡∏î"]
}

# Load models and data
def load_models_and_data(model_dir: str = "./data"):
    print("Loading models and data...", file=sys.stderr)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    try:
        if not os.path.exists(model_dir):
            print(f"Error: Directory '{model_dir}' not found. Please ensure your data is in this folder.", file=sys.stderr)
            return None, None, None, None, None, None, device
        
        # Load ChromaDB
        client = chromadb.PersistentClient(path=f"{model_dir}/chroma_db")
        collection = client.get_collection(name="thai_legal")
        
        # Load passages
        with open(f"{model_dir}/passages.pkl", "rb") as f:
            passages = pickle.load(f)
            
        # Load Sentence Transformer for dense retrieval
        retriever_model = SentenceTransformer("intfloat/multilingual-e5-small", device=device)
        
        # Prepare for BM25 (sparse retrieval)
        passage_texts = [p["text"] for p in passages]
        tokenized_corpus = [re.findall(r'[\u0E00-\u0E7F0-9a-zA-Z]+', text.lower()) for text in passage_texts]
        bm25_model = BM25Okapi(tokenized_corpus)
        
        # Load LLM (Tokenizer and LoRA) from Hugging Face
        llm_tokenizer = AutoTokenizer.from_pretrained("DuckerMaster/thai-legal-lora")
        llm_tokenizer.pad_token = llm_tokenizer.eos_token
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        base_model = AutoModelForCausalLM.from_pretrained(
            "scb10x/llama3.2-typhoon2-3b",
            torch_dtype=torch.bfloat16,
            device_map="auto",
            low_cpu_mem_usage=True,
            quantization_config=quantization_config
        )
        llm_model = PeftModel.from_pretrained(base_model, "DuckerMaster/thai-legal-lora")
        print("Models and data loaded successfully!", file=sys.stderr)
        return passages, collection, retriever_model, bm25_model, llm_model, llm_tokenizer, device
    except Exception as e:
        print(f"Error loading models/data: {e}", file=sys.stderr)
        return None, None, None, None, None, None, device

# Load models globally
passages, collection, retriever_model, bm25_model, llm_model, llm_tokenizer, device = load_models_and_data()
if not all([passages, collection, retriever_model, bm25_model, llm_model, llm_tokenizer]):
    print("Application cannot run due to failed component loading. Exiting.", file=sys.stderr)
    exit(1)

# Utility functions
def get_text(passage: dict) -> str:
    return passage.get('text', '') if isinstance(passage, dict) else str(passage)

def get_meta(passage: dict, key: str, default: str = '') -> str:
    return passage.get('metadata', {}).get(key, default)

def tokenize_text(text: str) -> list[str]:
    return re.findall(r'[\u0E00-\u0E7F0-9a-zA-Z]+', str(text).lower())

def check_passage_relevance(question: str, passage_text: str, model, tokenizer, device) -> tuple[bool, bool]:
    question_lower = question.lower()
    passage_lower = passage_text.lower()
    question_keywords = tokenize_text(question)
    passage_keywords = tokenize_text(passage_text)
    matching_keywords = set(question_keywords) & set(passage_keywords)
    is_relevant = len(matching_keywords) >= 2 or any(kw in passage_lower for kw in question_lower.split() if len(kw) > 2)
    has_steps = '‡∏ß‡∏¥‡∏ò‡∏µ' in question_lower and any(step_word in passage_lower for step_word in ['‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô', '‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£', '‡∏ï‡πâ‡∏≠‡∏á', '‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£'])
    return is_relevant, has_steps

# Retrieval
def retrieve_documents(query: str, collection, retriever_model, bm25_model, passages, device, k: int = 1):
    tokenized_query = tokenize_text(query)
    bm25_scores = bm25_model.get_scores(tokenized_query)
    top_bm25_indices = np.argsort(bm25_scores)[-min(100, len(passages)):]
    
    query_emb = retriever_model.encode([query], convert_to_numpy=True, device=device).astype('float32')
    chroma_results = collection.query(
        query_embeddings=query_emb.tolist(),
        n_results=k,
        include=["documents", "metadatas", "distances"]
    )
    
    results = []
    for doc, meta, dist in zip(chroma_results["documents"][0], chroma_results["metadatas"][0], chroma_results["distances"][0]):
        try:
            passage_idx = next(j for j, p in enumerate(passages) if p.get("text") == doc)
            dense_score = 1 - (dist / (max(chroma_results["distances"][0]) + 1e-6))
            sparse_score = bm25_scores[passage_idx] / (max(bm25_scores) + 1e-6 if max(bm25_scores) > 0 else 1e-6)
            combined_score = 0.8 * dense_score + 0.2 * sparse_score
            passage = passages[passage_idx]
            if any(kw in get_text(passage) for kw in LEGAL_KEYWORDS):
                combined_score += 0.3
            results.append({"passage": passage, "score": combined_score})
        except StopIteration:
            continue
    return sorted(results, key=lambda x: x["score"], reverse=True)[:k]

# Answer generation
def generate_llm_answer(question: str, passage: dict, score: float, model, tokenizer, device) -> str:
    if not model or not tokenizer:
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"
    
    is_relevant, has_steps = check_passage_relevance(question, get_text(passage), model, tokenizer, device)
    
    if score > 0.5 and is_relevant:
        context = f"{get_meta(passage, 'law_title')} ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ {get_meta(passage, 'section')}: {get_text(passage)}"
        if '‡∏ß‡∏¥‡∏ò‡∏µ' in question and not has_steps:
            prompt = (
                f"‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢: {context}\n\n"
                f"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\n"
                f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÉ‡∏´‡πâ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
            )
        else:
            prompt = f"‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢: {context}\n\n‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\n‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô"
        
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(device)
        with torch.amp.autocast('cuda') if device == "cuda" else torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, "").strip()
        start_index = response.rfind("‡∏ï‡∏≠‡∏ö‡πÇ‡∏î‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô")
        if start_index != -1:
            response = response[start_index:].strip()

        if not response:
            return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"
        
        return response
    return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"

# Main answer function
def get_answer(question: str, history):
    start_time = time.time()
    cleaned_question = question.strip()
    
    if not cleaned_question or len(cleaned_question) < 3 or not any(kw in cleaned_question.lower() for kw in LEGAL_KEYWORDS):
        output = f"üí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢\n‚è±Ô∏è Time taken: {time.time() - start_time:.2f} seconds"
        new_history = history + [{'role': 'user', 'content': cleaned_question}, {'role': 'assistant', 'content': output}]
        return "", new_history
    
    retrieved = retrieve_documents(question, collection, retriever_model, bm25_model, passages, device, k=1)
    if not retrieved:
        output = f"üí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á\n‚è±Ô∏è Time taken: {time.time() - start_time:.2f} seconds"
        new_history = history + [{'role': 'user', 'content': cleaned_question}, {'role': 'assistant', 'content': output}]
        return "", new_history
    
    top_passage = retrieved[0]['passage']
    score = retrieved[0]['score']
    response = generate_llm_answer(question, top_passage, score, llm_model, llm_tokenizer, device)
    
    if response == "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á":
        output = f"üí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n\n"
        output += f"üìã ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:\n ¬† üìñ {get_meta(top_passage, 'law_title')} ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ {get_meta(top_passage, 'section')}\n"
        output += f" ¬† üìù {get_text(top_passage)[:200]}...\n"
        output += f" ¬† ‚≠ê ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {score:.2f}\n\n"
        output += f"üí° ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡∏•‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô"
    else:
        output = f"üí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: {response}\n"
        output += f"üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤:\n ¬† 1. {get_meta(top_passage, 'law_title')} ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ {get_meta(top_passage, 'section')} (Score: {score:.2f})\n"
    
    output += f"‚è±Ô∏è Time taken: {time.time() - start_time:.2f} seconds"
    
    new_history = history + [{'role': 'user', 'content': cleaned_question}, {'role': 'assistant', 'content': output}]
    return "", new_history

# Gradio UI
with gr.Blocks(theme=gr.themes.Soft(), title="‡∏ö‡∏≠‡∏ó‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÑ‡∏ó‡∏¢", css=".gr-button {margin: 5px;} .gr-panel {font-size: 14px; overflow-y: auto; max-height: 600px;}") as demo:
    gr.Markdown("""
    # ü§ñ ‡∏ö‡∏≠‡∏ó‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÑ‡∏ó‡∏¢
    üí° ‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÑ‡∏ó‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
    üìö ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö 34 ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢ (‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏î‡πâ‡∏≤‡∏ô‡∏Ç‡∏ß‡∏≤)
    üîÑ ‡πÉ‡∏ä‡πâ‡∏õ‡∏∏‡πà‡∏° "‡∏•‡πâ‡∏≤‡∏á" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° ‡∏´‡∏£‡∏∑‡∏≠ "‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    üí° ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°, ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà ‡∏õ.‡∏õ.‡∏ä., ‡∏°‡∏±‡∏î‡∏à‡∏≥
    """)
    
    with gr.Row():
        with gr.Column(scale=3):
            question_input = gr.Textbox(label="üë§ ‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢", placeholder="‡πÄ‡∏ä‡πà‡∏ô ‡∏°‡∏±‡∏î‡∏à‡∏≥‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£", lines=2)
            with gr.Row():
                submit_button = gr.Button("‡∏™‡πà‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°", variant="primary")
                clear_button = gr.Button("‡∏•‡πâ‡∏≤‡∏á", variant="secondary")
                clear_history_button = gr.Button("‡∏•‡πâ‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥", variant="secondary")
            
            gr.Markdown("## üìú ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
            history_output = gr.Chatbot(label="‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤", height=300, type="messages", show_label=False)
            
            gr.Markdown("## üìã ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°")
            with gr.Column():
                for law, questions in example_questions.items():
                    with gr.Accordion(law, open=False):
                        for q in questions:
                            gr.Button(q, size="sm").click(
                                lambda q_text: q_text,
                                inputs=[gr.State(q)],
                                outputs=question_input,
                                queue=False,
                            ).then(
                                get_answer,
                                inputs=[question_input, history_output],
                                outputs=[question_input, history_output]
                            )
        
        with gr.Column(scale=1, variant="panel"):
            gr.Markdown("## üìö ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö")
            for law, desc in LEGAL_CATEGORIES.items():
                gr.Markdown(f"‚Ä¢ **{law}**: {desc}")

    submit_button.click(
        fn=get_answer,
        inputs=[question_input, history_output],
        outputs=[question_input, history_output]
    )
    
    clear_button.click(
        fn=lambda: ("", None),
        inputs=None,
        outputs=[question_input, history_output]
    )
    
    clear_history_button.click(
        fn=lambda: None,
        inputs=None,
        outputs=history_output
    )

if __name__ == "__main__":
    demo.launch()