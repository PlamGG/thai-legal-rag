import pickle
import chromadb
import numpy as np
import torch
import re
import sys
import logging
import os
import time
import json
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer
from peft import PeftModel
from rank_bm25 import BM25Okapi
from tqdm import tqdm

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# ‡∏õ‡∏¥‡∏î DEBUG_MODE ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á log ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏î‡∏µ‡∏ö‡∏±‡πä‡∏Å
DEBUG_MODE = False
if DEBUG_MODE:
    logging.getLogger().setLevel(logging.DEBUG)

# Global legal keywords
LEGAL_KEYWORDS = {
    '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó', '‡∏´‡∏∏‡πâ‡∏ô', '‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£', '‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢', '‡∏°‡∏≤‡∏ï‡∏£‡∏≤', '‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏ö‡∏±‡∏ç‡∏ç‡∏±‡∏ï‡∏¥', '‡∏à‡∏î‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô', '‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°', '‡∏†‡∏≤‡∏©‡∏µ', '‡∏Ñ‡∏∑‡∏ô‡∏†‡∏≤‡∏©‡∏µ', '‡∏¢‡∏∑‡πà‡∏ô‡∏Ñ‡∏≥‡∏£‡πâ‡∏≠‡∏á',
    '‡πÅ‡∏û‡πà‡∏á', '‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå', '‡∏£‡∏±‡∏©‡∏é‡∏≤‡∏Å‡∏£', '‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå', '‡∏î‡∏¥‡∏à‡∏¥‡∏ó‡∏±‡∏•', '‡∏ô‡∏¥‡∏ï‡∏¥‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•', '‡∏Å‡∏≠‡∏á‡∏ó‡∏∏‡∏ô', '‡∏™‡∏≥‡∏£‡∏≠‡∏á', '‡πÄ‡∏•‡∏µ‡πâ‡∏¢‡∏á‡∏ä‡∏µ‡∏û', '‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô',
    '‡∏£‡∏±‡∏ê‡∏°‡∏ô‡∏ï‡∏£‡∏µ', '‡∏à‡∏±‡∏î‡∏ã‡∏∑‡πâ‡∏≠', '‡∏à‡∏±‡∏î‡∏à‡πâ‡∏≤‡∏á', '‡∏û‡∏±‡∏™‡∏î‡∏∏', '‡∏†‡∏≤‡∏Ñ‡∏£‡∏±‡∏ê', '‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', '‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏Ñ‡∏ô‡∏ï‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ß', '‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à', '‡∏≠‡∏ô‡∏∏‡∏£‡∏±‡∏Å‡∏©‡πå',
    '‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô', '‡∏™‡∏°‡∏≤‡∏Ñ‡∏°', '‡∏°‡∏π‡∏•‡∏ô‡∏¥‡∏ò‡∏¥', '‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô', '‡∏ó‡∏£‡∏±‡∏™‡∏ï‡πå', '‡∏ò‡∏∏‡∏£‡∏Å‡∏£‡∏£‡∏°', '‡∏ï‡∏•‡∏≤‡∏î‡∏ó‡∏∏‡∏ô', '‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô', '‡∏ó‡∏∏‡∏ô', '‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à',
    '‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô', '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡πÅ‡∏´‡πà‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢', '‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏™‡∏ñ‡∏≤‡∏ö‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô', '‡∏ò‡∏ô‡∏≤‡∏Ñ‡∏≤‡∏£‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå', '‡πÄ‡∏Ñ‡∏£‡∏î‡∏¥‡∏ï‡∏ü‡∏≠‡∏á‡∏ã‡∏¥‡πÄ‡∏≠‡∏£‡πå', '‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå',
    '‡∏´‡∏∏‡πâ‡∏ô‡∏™‡∏≤‡∏°‡∏±‡∏ç', '‡∏´‡∏∏‡πâ‡∏ô‡∏ö‡∏∏‡∏£‡∏¥‡∏°‡∏™‡∏¥‡∏ó‡∏ò‡∏¥', '‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏´‡∏∏‡πâ‡∏ô', '‡∏Å‡∏≤‡∏£‡∏ñ‡∏∑‡∏≠‡∏´‡∏∏‡πâ‡∏ô', '‡πÄ‡∏á‡∏¥‡∏ô‡∏õ‡∏±‡∏ô‡∏ú‡∏•', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏à‡∏≥‡∏Å‡∏±‡∏î', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏°‡∏´‡∏≤‡∏ä‡∏ô‡∏à‡∏≥‡∏Å‡∏±‡∏î', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏•‡∏π‡∏Å',
    '‡πÉ‡∏ö‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï', '‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏≠‡∏á‡∏ó‡∏∏‡∏ô', '‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠', '‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏ù‡∏≤‡∏Å‡πÄ‡∏á‡∏¥‡∏ô', '‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏î‡∏°‡πÄ‡∏á‡∏¥‡∏ô', '‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏Å‡∏§‡∏©‡∏é‡∏µ‡∏Å‡∏≤', '‡∏£‡∏≤‡∏ä‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏ô‡∏∏‡πÄ‡∏ö‡∏Å‡∏©‡∏≤',
    '‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏î‡∏π‡πÅ‡∏•', '‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏á‡∏≤‡∏ô', '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏ä‡∏≠‡∏ö', '‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö', '‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏ç‡πà', '‡∏™‡∏≤‡∏Ç‡∏≤', '‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢', '‡∏Ñ‡πà‡∏≤‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£',
    '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á', '‡∏ô‡∏¥‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°', '‡∏™‡∏±‡∏ç‡∏ç‡∏≤', '‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï', '‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô', '‡∏õ‡∏£‡∏≤‡∏ö‡∏õ‡∏£‡∏≤‡∏°', '‡πÄ‡∏á‡∏¥‡∏ô‡πÑ‡∏î‡πâ', '‡∏õ‡∏¥‡πÇ‡∏ï‡∏£‡πÄ‡∏•‡∏µ‡∏¢‡∏°', '‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢', '‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏©‡∏ï‡∏£',
    '‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤', '‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û', '‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì', '‡∏ß‡∏¥‡∏ô‡∏±‡∏¢', '‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô', '‡∏Ñ‡∏•‡∏±‡∏á', '‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î', '‡∏Ç‡πâ‡∏≤‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£',
    '‡∏û‡∏±‡∏í‡∏ô‡∏≤', '‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à', '‡∏™‡∏±‡∏á‡∏Ñ‡∏°', '‡∏´‡∏•‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô', '‡∏´‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤', '‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô', '‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå', '‡∏õ.‡∏õ.‡∏ä.', '‡πÅ‡∏™‡∏ï‡∏°‡∏õ‡πå', '‡πÉ‡∏ö‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏©‡∏µ',
    '‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡πÄ‡∏â‡∏û‡∏≤‡∏∞', '‡∏≠‡∏ô‡∏∏‡∏£‡∏±‡∏Å‡∏©‡πå‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô', '‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', '‡∏™‡∏†‡∏≤‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≤‡∏£', '‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à', '‡∏ß‡∏¥‡∏ô‡∏±‡∏¢‡∏Ç‡πâ‡∏≤‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£',
    '‡∏û‡∏£‡∏∞‡∏£‡∏≤‡∏ä‡∏Å‡∏≥‡∏´‡∏ô‡∏î', '‡πÅ‡∏õ‡∏•‡∏á‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå', '‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå', '‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏¥‡∏à‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô', '‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡∏Ñ‡∏ô‡∏ï‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ß',
    '‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏Å‡∏±‡∏ö‡∏î‡∏π‡πÅ‡∏•‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à', '‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏≠‡∏ô‡∏∏‡∏£‡∏±‡∏Å‡∏©‡πå„Ç®„Éç„É´„ÇÆ„Éº', '‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏´‡πâ‡∏≤‡∏á‡∏´‡∏∏‡πâ‡∏ô‡∏™‡πà‡∏ß‡∏ô', '‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à',
    '‡∏ó‡∏£‡∏±‡∏™‡∏ï‡πå‡∏ï‡∏•‡∏≤‡∏î‡∏ó‡∏∏‡∏ô', '‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏û‡∏≤‡∏ì‡∏¥‡∏ä‡∏¢‡πå', '‡∏ó‡∏∏‡∏ô‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à', '‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡∏°‡∏´‡∏≤‡∏ä‡∏ô‡∏à‡∏≥‡∏Å‡∏±‡∏î', '‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏£‡∏≤‡∏ö‡∏õ‡∏£‡∏≤‡∏°‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï',
    '‡∏†‡∏≤‡∏©‡∏µ‡πÄ‡∏á‡∏¥‡∏ô‡πÑ‡∏î‡πâ‡∏õ‡∏¥‡πÇ‡∏ï‡∏£‡πÄ‡∏•‡∏µ‡∏¢‡∏°', '‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤', '‡∏ß‡∏¥‡∏ä‡∏≤‡∏ä‡∏µ‡∏û‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', '‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡∏á‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì', '‡∏ß‡∏¥‡∏ô‡∏±‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏±‡∏á',
    '‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ê', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏ß‡∏¥‡∏ô‡∏±‡∏¢‡∏Ç‡πâ‡∏≤‡∏£‡∏≤‡∏ä‡∏Å‡∏≤‡∏£', '‡∏™‡∏†‡∏≤‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏™‡∏±‡∏á‡∏Ñ‡∏°', '‡∏™‡∏°‡∏≤‡∏Ñ‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤',
    '‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤', '‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏ï‡∏•‡∏≤‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå', '‡∏´‡∏•‡∏±‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à', '‡∏´‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤', '‡πÅ‡∏£‡∏á‡∏á‡∏≤‡∏ô‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå',
    '‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£', '‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£', '‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô', '‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î', '‡∏™‡∏¥‡∏ó‡∏ò‡∏¥', '‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà', '‡∏ö‡∏ó‡∏•‡∏á‡πÇ‡∏ó‡∏©', '‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥', '‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£', '‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á', '‡∏°‡∏±‡∏î‡∏à‡∏≥'
}

# Cache for retrieved passages and LLM outputs
cache = {}
embedding_cache = {}

# 1. Configuration & Model Loading
def load_models_and_data(model_dir: str = "./data"):
    torch.cuda.empty_cache()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"Using device: {device}")
    if device == "cpu":
        logger.warning("GPU not available, falling back to CPU with full-precision model. Performance may be slower.")

    try:
        client = chromadb.PersistentClient(path=f"{model_dir}/chroma_db")
        collection = client.get_collection(name="thai_legal")
    except Exception as e:
        logger.error(f"Error loading ChromaDB collection: {e}")
        return (None,) * 7

    try:
        with open(f"{model_dir}/passages.pkl", "rb") as f:
            passages = pickle.load(f)
        logger.info(f"Loaded {len(passages)} passages.")
    except Exception as e:
        logger.error(f"Error loading passages file: {e}")
        return (None,) * 7

    retriever_model = SentenceTransformer("intfloat/multilingual-e5-small", device=device)
    passage_texts = [p["text"] for p in passages]
    tokenized_corpus = [re.findall(r'[\u0E00-\u0E7F0-9a-zA-Z]+', text.lower()) for text in passage_texts]
    bm25_model = BM25Okapi(tokenized_corpus)

    llm_model, llm_tokenizer = None, None
    try:
        llm_tokenizer = AutoTokenizer.from_pretrained(f"{model_dir}/trained_lora_model", local_files_only=True)
        llm_tokenizer.pad_token = llm_tokenizer.eos_token
        if device == "cuda":
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            base_model = AutoModelForCausalLM.from_pretrained(
                "scb10x/llama3.2-typhoon2-3b",
                torch_dtype=torch.bfloat16,
                device_map="auto",
                low_cpu_mem_usage=True,
                quantization_config=quantization_config
            )
        else:
            base_model = AutoModelForCausalLM.from_pretrained(
                "scb10x/llama3.2-typhoon2-3b",
                torch_dtype=torch.float32,
                device_map="cpu",
                low_cpu_mem_usage=True
            )
        llm_model = PeftModel.from_pretrained(base_model, f"{model_dir}/trained_lora_model")
        dummy_input = llm_tokenizer("‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ", return_tensors="pt").to(device)
        _ = llm_model.generate(**dummy_input, max_new_tokens=10)
        logger.info("Models loaded successfully.")
    except Exception as e:
        logger.error(f"Failed to load LLM: {e}")
        return (None,) * 7

    return passages, collection, retriever_model, bm25_model, llm_model, llm_tokenizer, device

# 2. Utility Functions
def get_text(passage: dict) -> str:
    return passage.get('text', '') if isinstance(passage, dict) else str(passage)

def get_meta(passage: dict, key: str, default: any = '') -> any:
    meta = passage.get('metadata', {}) if isinstance(passage, dict) else {}
    return meta.get(key, default) if isinstance(meta, dict) else default

def tokenize_text(text: str) -> list[str]:
    return re.findall(r'[\u0E00-\u0E7F0-9a-zA-Z]+', str(text).lower())

def calc_similarity(queries: list, texts: list, retriever_model, device) -> np.ndarray:
    cache_key = tuple(queries + texts)
    if cache_key in embedding_cache:
        return embedding_cache[cache_key]
    if not queries or not texts:
        return np.zeros((len(queries), len(texts)))
    with torch.amp.autocast(device_type="cuda" if device.startswith("cuda") else "cpu", dtype=torch.bfloat16):
        embeddings = retriever_model.encode(queries + texts, convert_to_numpy=True, device=device, batch_size=16)
        query_embs = embeddings[:len(queries)]
        text_embs = embeddings[len(queries):]
        scores = np.dot(query_embs, text_embs.T) / (np.linalg.norm(query_embs, axis=1)[:, None] * np.linalg.norm(text_embs, axis=1)[None, :] + 1e-6)
        for i, text in enumerate(texts):
            if any(kw in tokenize_text(text) for kw in LEGAL_KEYWORDS):
                scores[:, i] += 0.3
    embedding_cache[cache_key] = scores.clip(0, 1)
    return embedding_cache[cache_key]

def complete_last_sentence(text: str) -> str:
    if not text:
        return "."
    sentences = re.split(r'[.!?„ÄÇ]', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    if not sentences:
        return text + "."
    last_sentence = sentences[-1]
    if not last_sentence.endswith(('.','!','?','„ÄÇ')):
        return text + " ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≥‡∏´‡∏ô‡∏î."
    return text

# 3. RAG Core Functions
def retrieve_documents(query: str, collection, retriever_model, bm25_model, passages, device, k: int = 1):
    try:
        logger.info(f"Retrieving for query: {query}")
        tokenized_query = tokenize_text(query)
        bm25_scores = bm25_model.get_scores(tokenized_query)
        top_bm25_indices = np.argsort(bm25_scores)[-100:]
        filtered_passages = [passages[i] for i in top_bm25_indices]
        with torch.amp.autocast(device_type="cuda" if device.startswith("cuda") else "cpu", dtype=torch.bfloat16):
            query_emb = retriever_model.encode([query], convert_to_numpy=True, device=device).astype('float32')
        chroma_results = collection.query(
            query_embeddings=query_emb.tolist(),
            n_results=k,
            include=["documents", "metadatas", "distances"]
        )
        results = []
        for doc, meta, dist in zip(chroma_results["documents"][0], chroma_results["metadatas"][0], chroma_results["distances"][0]):
            try:
                passage_idx = next(j for j, p in enumerate(passages) if p.get("text") == doc)
                dense_score = 1 - (dist / (max(chroma_results["distances"][0]) + 1e-6))
                sparse_score = bm25_scores[passage_idx] / (max(bm25_scores) + 1e-6 if max(bm25_scores) > 0 else 1e-6)
                combined_score = 0.8 * dense_score + 0.2 * sparse_score
                passage = passages[passage_idx]
                if any(kw in get_text(passage) for kw in LEGAL_KEYWORDS):
                    combined_score += 0.3
                results.append({"passage": passage, "score": combined_score})
            except StopIteration:
                continue
        sorted_results = sorted(results, key=lambda x: x["score"], reverse=True)[:k]
        logger.info(f"Found {len(sorted_results)} results, top score: {sorted_results[0]['score'] if sorted_results else 0}")
        return sorted_results
    except Exception as e:
        logger.error(f"Retrieval error: {e}")
        return []

def check_passage_relevance(question: str, passage_text: str, model, tokenizer, device) -> tuple[bool, bool]:
    try:
        prompt = (
            f"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\n"
            f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {passage_text}\n"
            f"‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ {{'is_relevant': bool, 'has_steps': bool}} ‡πÇ‡∏î‡∏¢:\n"
            f"- 'is_relevant': true ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°, false ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á\n"
            f"- 'has_steps': true ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô, false ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ\n"
            f"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:\n"
            f"1. ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: '‡∏°‡∏±‡∏î‡∏à‡∏≥‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£' ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '‡∏°‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏±‡πâ‡∏ô ‡∏ñ‡πâ‡∏≤‡∏°‡∏¥‡πÑ‡∏î‡πâ‡∏ï‡∏Å‡∏•‡∏á...' ‚Üí {{'is_relevant': true, 'has_steps': false}}\n"
            f"2. ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: '‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó' ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏¢‡∏∑‡πà‡∏ô...' ‚Üí {{'is_relevant': true, 'has_steps': true}}\n"
            f"3. ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: '‡∏°‡∏±‡∏î‡∏à‡∏≥‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£' ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: '‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏Ñ‡∏∑‡∏≠...' ‚Üí {{'is_relevant': false, 'has_steps': false}}\n"
            f"‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ JSON ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö {{}} ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"
        )
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256).to(device)
        with torch.amp.autocast(device_type="cuda" if device.startswith("cuda") else "cpu", dtype=torch.bfloat16):
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, "").strip()

        # ‡πÉ‡∏ä‡πâ regex ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô JSON
        json_match = re.search(r'\{.*?\}', response, re.DOTALL)
        if json_match:
            try:
                result = json.loads(json_match.group(0))
                is_relevant = result.get('is_relevant', False)
                has_steps = result.get('has_steps', False)
                return is_relevant, has_steps
            except json.JSONDecodeError:
                logger.error(f"JSON decode error: {json_match.group(0)}")
                pass

    except Exception as e:
        logger.error(f"Relevance check error: {e}")
    
    # Fallback: ‡πÉ‡∏ä‡πâ heuristic-based relevance checking
    question_lower = question.lower()
    passage_lower = passage_text.lower()
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö keyword matching
    question_keywords = tokenize_text(question)
    passage_keywords = tokenize_text(passage_text)
    
    # ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
    matching_keywords = set(question_keywords) & set(passage_keywords)
    
    is_relevant = len(matching_keywords) >= 2 or any(
        kw in passage_lower for kw in question_lower.split() if len(kw) > 2
    )
    
    has_steps = '‡∏ß‡∏¥‡∏ò‡∏µ' in question_lower and any(
        step_word in passage_lower for step_word in ['‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô', '‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£', '‡∏ï‡πâ‡∏≠‡∏á', '‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£', '‡∏à‡∏±‡∏î‡∏ó‡∏≥']
    )
    
    logger.warning(f"Fallback to keyword-based relevance: is_relevant={is_relevant}, has_steps={has_steps}")
    return is_relevant, has_steps

def generate_llm_answer(question: str, context: str, passage: dict, score: float, model, tokenizer, device) -> str:
    if not model or not tokenizer:
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"
    try:
        is_relevant, has_steps = check_passage_relevance(question, get_text(passage), model, tokenizer, device)
        if score > 0.5 and is_relevant:  # ‡∏•‡∏î threshold ‡∏à‡∏≤‡∏Å 0.7 ‡πÄ‡∏õ‡πá‡∏ô 0.5
            if '‡∏ß‡∏¥‡∏ò‡∏µ' in question and not has_steps:
                prompt = (
                    f"‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢: {context}\n\n"
                    f"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}\n"
                    f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÉ‡∏´‡πâ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏à‡∏ö‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå ‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô"
                )
                inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=256).to(device)
                with torch.amp.autocast(device_type="cuda" if device.startswith("cuda") else "cpu", dtype=torch.bfloat16):
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=200,
                        pad_token_id=tokenizer.eos_token_id,
                        eos_token_id=tokenizer.eos_token_id,
                        repetition_penalty=1.1
                    )
                response = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, "").strip()
                response = re.sub(r'‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡πâ‡∏ô‡πÜ.*$', '', response, flags=re.DOTALL)
                return complete_last_sentence(response)
            else:
                return complete_last_sentence(get_text(passage))
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"
    except Exception as e:
        logger.error(f"LLM error: {e}")
        # ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î error ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á enough ‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• anyway
        if score > 0.4:  # threshold ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤
            return f"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {get_text(passage)[:300]}..."
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"
    finally:
        torch.cuda.empty_cache()

def get_answer(question: str, passages, collection, retriever_model, bm25_model, llm_model, llm_tokenizer, device):
    start_time = time.time()
    
    # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç validation - ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢
    cleaned_question = question.strip()
    if len(cleaned_question) < 3:
        return f"üí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô\n‚è±Ô∏è Time taken: {time.time() - start_time:.2f} seconds", []
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    thai_chars = sum(1 for c in cleaned_question if '\u0E00' <= c <= '\u0E7F')
    if thai_chars < 1 and not any(kw in cleaned_question.lower() for kw in LEGAL_KEYWORDS):
        return f"üí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢\n‚è±Ô∏è Time taken: {time.time() - start_time:.2f} seconds", []
    
    if question in cache:
        retrieved, cached_response = cache[question]
        if cached_response:
            return f"{cached_response}\n‚è±Ô∏è Time taken: {time.time() - start_time:.2f} seconds", retrieved
    else:
        retrieved = retrieve_documents(question, collection, retriever_model, bm25_model, passages, device, k=3)  # ‡πÄ‡∏û‡∏¥‡πà‡∏° k=3
        cache[question] = (retrieved, None)
    
    if not retrieved:
        return f"üí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á\n‚è±Ô∏è Time taken: {time.time() - start_time:.2f} seconds", []
    
    # Generate LLM answer
    top_passage = retrieved[0]['passage']
    score = retrieved[0]['score']
    context = f"{get_meta(top_passage, 'law_title')} ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ {get_meta(top_passage, 'section')}: {get_text(top_passage)}"
    response = generate_llm_answer(question, context, top_passage, score, llm_model, llm_tokenizer, device)
    
    # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ: ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏°‡πâ‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡πÄ‡∏õ‡πä‡∏∞ ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á
    if response == "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á" and retrieved:
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠
        best_match = retrieved[0]
        best_text = get_text(best_match['passage'])
        best_law = get_meta(best_match['passage'], 'law_title', '‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢')
        best_section = get_meta(best_match['passage'], 'section', '‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏≤')
        
        output = f"üí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: ‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞\n\n"
        output += f"üìã ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö:\n"
        output += f"   üìñ {best_law} ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ {best_section}\n"
        output += f"   üìù {best_text[:200]}..." if len(best_text) > 200 else f"   üìù {best_text}\n\n"
        output += f"   ‚≠ê ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {best_match['score']:.2f}\n\n"
        output += f"üí° ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡∏¢‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á"
    else:
        output = f"üí° ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢: {response}\n"
        if response != "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á":
            output += f"üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤:\n   1. {get_meta(top_passage, 'law_title')} ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ {get_meta(top_passage, 'section')} (Score: {score:.2f})\n"
    
    output += f"‚è±Ô∏è Time taken: {time.time() - start_time:.2f} seconds"
    cache[question] = (retrieved, output)
    return output, retrieved

# 4. Main Application & User Interface
def main():
    passages, collection, retriever_model, bm25_model, llm_model, llm_tokenizer, device = load_models_and_data()
    if not all([passages, collection, retriever_model, bm25_model, llm_model, llm_tokenizer]):
        logger.error("Application cannot run due to failed component loading.")
        sys.exit(1)

    print("üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö (‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö)")
    test_questions = [
        # "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£",
        # "‡∏Å‡∏≤‡∏£‡∏¢‡∏∑‡πà‡∏ô‡∏†‡∏≤‡∏©‡∏µ‡πÄ‡∏á‡∏¥‡∏ô‡πÑ‡∏î‡πâ‡∏õ‡∏¥‡πÇ‡∏ï‡∏£‡πÄ‡∏•‡∏µ‡∏¢‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£",
        # "‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏±‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á‡∏£‡∏±‡∏ê‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£",
        # "‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏õ‡∏ä",
        # "‡∏™‡∏°‡∏≤‡∏Ñ‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£"
        "‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°",
        "‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï‡πÉ‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏£‡∏±‡∏ê",
    ]
    for i, q in enumerate(test_questions, 1):
        print(f"\n‚ùì ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà {i}: {q}")
        resp, sources = get_answer(q, passages, collection, retriever_model, bm25_model, llm_model, llm_tokenizer, device)
        print(resp)
        print("---")
    
    print("ü§ñ ‡∏ö‡∏≠‡∏ó‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÑ‡∏ó‡∏¢")
    print("üí° ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó, ‡∏´‡∏∏‡πâ‡∏ô, ‡∏Å‡∏£‡∏£‡∏°‡∏Å‡∏≤‡∏£, ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ä‡∏∏‡∏°, ‡∏†‡∏≤‡∏©‡∏µ, ‡∏ó‡∏∏‡∏à‡∏£‡∏¥‡∏ï, ‡∏£‡∏±‡∏ê‡∏ß‡∏¥‡∏™‡∏≤‡∏´‡∏Å‡∏¥‡∏à, ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô, ‡∏™‡∏°‡∏≤‡∏Ñ‡∏°, ‡∏ö‡∏±‡∏ç‡∏ä‡∏µ, ‡∏Å‡∏≤‡∏£‡∏ö‡∏£‡∏¥‡∏´‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£, ‡∏°‡∏±‡∏î‡∏à‡∏≥, ‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏µ‡∏¢‡∏†‡∏≤‡∏©‡∏µ‡∏≠‡∏≤‡∏Å‡∏£")
    print("‡∏û‡∏¥‡∏°‡∏û‡πå 'exit' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å\n" + "="*50)
    while True:
        user_input = input("üë§ ‡∏ñ‡∏≤‡∏°‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢: ").strip()
        if user_input.lower() in ["exit", "‡∏≠‡∏≠‡∏Å", "quit"]:
            print("‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ üòä")
            break
        if not user_input:
            continue
        resp, sources = get_answer(user_input, passages, collection, retriever_model, bm25_model, llm_model, llm_tokenizer, device)
        print("\n" + resp)
        print("-"*50)

if __name__ == "__main__":
    main()